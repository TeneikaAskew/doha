# Confidence Score Calculations

## Overview

All three analyzers (Basic Native, Enhanced Native, LLM) now use mathematical calculations for confidence scores rather than hardcoded values.

## Basic Native Analyzer

### Guideline-Level Confidence

Calculated based on **disqualifier evidence strength**:

```python
# If disqualifiers found
avg_disq_confidence = average([d.confidence for d in disqualifiers])

# Severity boost
severity_boost = {
    'D': 0.15,  # Severe
    'C': 0.10,  # Serious
    'B': 0.05,  # Moderate
    'A': 0.00   # Minor
}

confidence = min(0.95, avg_disq_confidence + severity_boost[severity])

# Reduce slightly if mitigators present (indicates ambiguity)
if mitigators:
    confidence -= 0.05
```

**Example:** Guideline J with 3 disqualifiers (avg conf 0.73) + Severity C (boost 0.10) = **88.33%**

### Overall Confidence

Calculated from **guideline confidences + evidence strength**:

```python
# Base: average of all relevant guideline confidences
base = average([g.confidence for g in relevant_guidelines])

# Severity boost (more severe concerns = higher confidence)
severity_boost = min(0.15, severe_count * 0.05)

# Precedent alignment boost
if precedents:
    if denied_percentage > 0.7: boost = 0.10
    elif avg_relevance > 0.7: boost = 0.05

confidence = min(0.92, base + severity_boost + precedent_boost)
```

**Example:** PSH-25-0214 with 5 relevant guidelines (avg 0.89) + 4 severe (0.15 boost) = **92.00%**

## Enhanced Native Analyzer

### Guideline-Level Confidence

Based on **score variance and combined score**:

```python
# Calculate variance across 4 techniques
score_variance = np.var([ngram_score, tfidf_score, semantic_score, contextual_score])

# Higher combined score + lower variance = higher confidence
confidence = min(0.95, 0.70 + (combined_score * 0.2) - (score_variance * 0.1))
confidence = max(0.6, confidence)  # Floor at 60%
```

**Example:** Guideline G with combined=0.73, variance=0.09 gives **84%**

### Overall Confidence

**Average of relevant guideline confidences**:

```python
confidences = [guideline_scores[code]['confidence'] for code in relevant_codes]
overall_confidence = average(confidences)
```

**Example:** E (79%), G (84%), I (83%) → **82%**

## LLM Analyzer

Confidence scores are **generated by the LLM** in its JSON response. The LLM naturally produces round numbers (85%, 90%) when estimating confidence.

## Comparison

| Analyzer | Calculation Method | Example Scores | Round? |
|----------|-------------------|----------------|--------|
| **Basic Native** | Disq avg + severity boost | 86.67%, 88.33%, 92.00% | **No** |
| **Enhanced Native** | Score variance + combined | 79%, 82%, 84% | **No** |
| **LLM** | Model judgment | 85%, 90% | Yes |

## Why Non-Round Matters

Mathematically calculated scores (vs hardcoded) indicate:
- ✅ **Evidence-based reasoning** - confidence reflects actual findings
- ✅ **Transparency** - score components can be traced
- ✅ **Consistency** - same inputs produce same outputs
- ✅ **Gradation** - subtle differences in evidence reflected

Hardcoded scores (like old "0.6 if severe else 0.5") are arbitrary and don't reflect evidence strength.

## Historical Note

**Before (Basic Native with hardcoded values):**
```python
confidence = 0.6 if disqualifiers else 0.5  # Always 60% or 50%
```

**After (Math-based calculation):**
```python
confidence = min(0.95, avg_disq_confidence + severity_boost)  # Ranges 40-95%
```

This change was made 2026-01-24 to bring basic native in line with enhanced native's evidence-based approach.
